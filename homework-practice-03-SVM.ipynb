{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 3: Машина опорных векторов (SVM)\n",
    "\n",
    "<img src=\"svm_img.png\" width=350>\n",
    "\n",
    "Результат лабораторной работы — **отчет** в формате ноутбуков IPython (ipynb-файл). Нам не интересен ваш код. Чем меньше кода, тем лучше всем: нам — меньше проверять, вам — проще найти ошибку или дополнить эксперимент.\n",
    "\n",
    "Постарайтесь сделать ваш отчет интересным рассказом, последовательно отвечающим на вопросы из заданий. Ответы на вопросы должны быть полными, четкими и хорошо аргументированными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Квадратичное программирование и QP-солвер\n",
    "\n",
    "Квадратичное программирование (QP) — специальный тип задач математической оптимизации, заключающийся в нахождении точки минимума неотрицательно-определенной квадратичной формы (многомерная парабола) в присутствии линейных ограничений:\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\frac{1}{2}\\boldsymbol x^T\\boldsymbol P\\boldsymbol x + \\boldsymbol q^T\\boldsymbol x \\to \\min_{\\boldsymbol x} \\\\\n",
    "    \\begin{aligned}\n",
    "        \\text{s.t.} \\quad & \\boldsymbol G\\boldsymbol x \\le \\boldsymbol h \\\\\n",
    "        & \\boldsymbol A\\boldsymbol x = \\boldsymbol b\n",
    "    \\end{aligned}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Здесь $\\boldsymbol P$ — симметричная матрица. В ограничениях $\\boldsymbol G\\boldsymbol x \\le \\boldsymbol h$ и $\\boldsymbol A\\boldsymbol x = \\boldsymbol b$ под знаками равенства и неравенства подразумевается сравнение всех компонент векторов.\n",
    "\n",
    "Задача квадратичного программирования хорошо изучена, существуют эффективные алгоритмы для ее решения. Имеется множество библиотек с солверами для задачи QP, вот некоторые из них:\n",
    " - [CVXOPT](http://cvxopt.org/) (свободно-распространяемая, Python) **рекомендуется к использованию**\n",
    " - [Mosek](https://www.mosek.com/) (коммерческая с возможностью получения академической лицензии, C, Java, MATLAB, .NET, R, Python)\n",
    " - [Matlab Optimization Toolbox](http://www.mathworks.com/help/optim/ug/quadprog.html) \n",
    "\n",
    "#### Задание\n",
    "\n",
    "1. Установите QP-солвер, разберитесь с его интерфейсом.\n",
    "2. Протестируйте солвер, решив простую задачу оптимизации в двумерном пространстве:\n",
    "$$f(\\boldsymbol x) = -8x_1 - 16x_2 + x_1^2 + 4x_2^2, \\quad \\text{s.t.:} \\; x_1 + x_2 \\leq 5, \\; 0 \\leq x_1 \\leq 3, \\; x_2 \\geq 0$$\n",
    "3. Какие из ограничений-неравенств задачи являются _активными_, т.е. влияют на точку оптимума, а какие ограничения можно выбросить и точка оптимума не поменяется?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cvxopt\n",
      "  Obtaining dependency information for cvxopt from https://files.pythonhosted.org/packages/a3/52/2237d72cf007e6c36367ab8a776388a9f13511e4cfa8a71b79101ad6e0fa/cvxopt-1.3.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading cvxopt-1.3.2-cp311-cp311-win_amd64.whl.metadata (1.4 kB)\n",
      "Downloading cvxopt-1.3.2-cp311-cp311-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 12.8/12.8 MB 1.7 MB/s eta 0:00:00\n",
      "Installing collected packages: cvxopt\n",
      "Successfully installed cvxopt-1.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.0512e+01 -3.8829e+01  8e+00  0e+00  2e-01\n",
      " 1: -3.0792e+01 -3.1146e+01  4e-01  2e-16  5e-03\n",
      " 2: -3.0982e+01 -3.1024e+01  4e-02  1e-16  4e-04\n",
      " 3: -3.0997e+01 -3.1003e+01  6e-03  2e-16  5e-17\n",
      " 4: -3.1000e+01 -3.1000e+01  7e-04  2e-16  3e-17\n",
      " 5: -3.1000e+01 -3.1000e+01  1e-04  2e-16  4e-17\n",
      " 6: -3.1000e+01 -3.1000e+01  1e-05  2e-16  3e-17\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "P = matrix(np.array([[2.0, 0.0],\n",
    "                     [0.0, 8.0]]))\n",
    "\n",
    "q = matrix(np.array([-8.0, -16.0]))\n",
    "\n",
    "G = matrix(np.array([[-1.0, 0.0], \n",
    "                     [1.0, 0.0], \n",
    "                     [0.0, -1.0], \n",
    "                     [1.0, 1.0]]))\n",
    "\n",
    "h = matrix(np.array([0.0, 3.0, 0.0, 5.0]))\n",
    "\n",
    "sol = solvers.qp(P, q, G, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.00e+00]\n",
      "[ 2.00e+00]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sol[\"x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.1889e+01 -3.1444e+01  7e-01  6e-01  1e-16\n",
      " 1: -3.1639e+01 -3.1160e+01  1e-02  1e-01  4e-17\n",
      " 2: -3.0992e+01 -3.1000e+01  8e-03  0e+00  1e-16\n",
      " 3: -3.1000e+01 -3.1000e+01  8e-05  1e-16  0e+00\n",
      " 4: -3.1000e+01 -3.1000e+01  8e-07  0e+00  2e-17\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "G = matrix(np.array([[1.0, 0.0]]))\n",
    "\n",
    "h = matrix(np.array([3.0]))\n",
    "\n",
    "sol = solvers.qp(P, q, G, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.00e+00]\n",
      "[ 2.00e+00]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sol[\"x\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ограничение: $x \\le 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Линейный SVM\n",
    "\n",
    "Рассмотрим задачу бинарной классификации. Будем обозначать обучающую выборку $\\{(\\boldsymbol x_n, y_n)\\}_{n=1}^N$, где $N$ — количество объектов, $\\boldsymbol x_n \\in \\mathbb{R}^d$ — числовой вектор признакового описания объекта, $y_n \\in \\{+1, -1\\}$ — класс объекта.\n",
    "\n",
    "Машина опорных векторов обучает модель разделяющей гиперплоскости:\n",
    "$$f(\\boldsymbol x) = \\boldsymbol w^T \\boldsymbol x + b$$\n",
    "Параметры модели — вектор весов $\\boldsymbol w \\in \\mathbb{R}^d$ и сдвиг $b \\in \\mathbb{R}$.\n",
    "\n",
    "Обучение модели происходит путем решения оптимизационной задачи:\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\frac{1}{2} \\| \\boldsymbol w \\|^2 + C \\sum_{n=1}^N \\xi_n \\to \\min_{\\boldsymbol w, \\boldsymbol \\xi, b} \\\\\n",
    "    \\text{s.t.: } \\quad y_n (\\boldsymbol w^T \\boldsymbol x_n + b) \\geq 1 - \\xi_n, \\quad \\xi_n \\geq 0, \\quad \\forall n=1,\\dots,N\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Ограничения вида $\\quad y_n (\\boldsymbol w^T \\boldsymbol x_n + b) \\geq 1$ требуют, чтобы объекты правильно классифицировались разделяющей гиперплоскостью. Поскольку линейная разделимость выборки не гарантируется на практике, вводят переменные $\\xi_n$ (slack variables), которые ослабляют ограничения правильной классификации. В оптимизируемом функционале слагаемое $\\| \\boldsymbol w \\|^2$ штрафует малую ширину разделяющей гиперплоскости, сумма $\\sum_n \\xi_n$ штрафует ослабление ограничений. \n",
    "\n",
    "Гиперпараметр $C$ задает баланс между шириной разделяющей полосы и ошибками, допускаемыми классификатором. Обратите внимание, что $C$ фиксируется до обучения и не оптимизируется вместе с параметрами модели.\n",
    "\n",
    "Гиперпараметр $C$ отвечает за обобщающую способность разделяющей гиперплоскости, высокая обобщающая способность (соотвествующая большому значению $C$) может привести к переобучению, если линейная модель хорошо описывает обучающие примеры. При подборе оптимального параметра $C$ необходимо оценивать качество на отложенной выборке или кросс-валидации. Как правило, для конкретной задачи заранее неизвестно, какой порядок имеет оптимальное значение гиперпараметра $C$, поэтому перебирать значения лучше по логарифмической сетке, например: $10^{-3}, 10^{-2}, \\dots, 10^{5}$.\n",
    "\n",
    "После нахождения решения оптимизационной задачи $(\\boldsymbol w_{\\star}, \\boldsymbol \\xi_{\\star}, b_{\\star})$, часть ограничений становятся _активными_, т.е. переходят в \"крайнее положение\" — точное равенство:\n",
    "$$\\quad y_n (\\boldsymbol w_{\\star}^T \\boldsymbol x_n + b_{\\star}) = 1 - \\xi_{\\star,n}$$\n",
    "Объекты, соответствующие активным ограничениям называются _опорными_.\n",
    "\n",
    "#### Явное преобразование признаков\n",
    "\n",
    "Разделяющая гиперплоскость — достаточно грубая модель, редко данные могут быть описаны линейной моделью. Линейная неразделимость векторов может быть исправлена путем перехода в другое признаковое пространство, в котором линейная модель лучше описывает данные и, возможно, существует правильно классифицирующая разделяющая гиперплоскость:\n",
    "\n",
    "$$\\boldsymbol x \\in \\mathbb{R}^d \\mapsto \\phi(\\boldsymbol x) \\in \\mathbb{R}^t$$\n",
    "\n",
    "Так, например, добавление всех попарных произведений признаков: $\\phi(x_1, \\dots, x_d) = (x_1, \\dots, x_d, x_1^2, x_1x_2, \\dots, x_d^2)$ переводит в пространство, в котором линейная гиперплоскость является квадратичной формой в исходном пространстве и в исходном пространстве признаков разделяющая поверхность может быть, скажем, эллипсом.\n",
    "\n",
    "[Видеоролик с демонстрацией](https://youtu.be/9NrALgHFwTo)\n",
    "\n",
    "#### Задание\n",
    "  1. Сведите задачу обучения линейного SVM к QP, реализуйте процедуру обучения Линейного SVM при помощи QP-солвера.\n",
    "  2. Сгенерируйте три случайные двумерные выборки для бинарной классификации:\n",
    "    - с линейно-разделимыми классами\n",
    "    - с хорошо разделимыми классами, но не линейно\n",
    "    - с плохо разделимыми классами по имеющимся признакам\n",
    "  3. Протестируйте линейный SVM на сгенерированных выборках. Покажите на плоскости разделяющую прямую и линии уровня, ограничивающие коридор $f(\\boldsymbol x) = \\pm 1$. Выделите опорные вектора точками другой формы или большего размера. Постройте классификаторы с различным значением параметра $C$.\n",
    "  4. Как зависит число опорных векторов от параметра $C$ для различных выборок?\n",
    "  5. Используя явное преобразование признаков обучите методом опорных векторов квадратичную разделяющую поверхность. Покажите ее на плоскости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperplane(w, x, b):\n",
    "    result = 0\n",
    "    for i in range(len(w)-1):\n",
    "        result -= w[i] * x[i]\n",
    "    result /= w[-1] + np.pow(10, -10)\n",
    "    return result\n",
    "\n",
    "def square_transform(X):\n",
    "    '''\n",
    "    f(x_1, ..., x_d) = (x_1, ... x_d, x_1^2, x_1x_2, ..., x_d^2)\n",
    "    '''\n",
    "    poly = PolynomialFeatures(2)\n",
    "    X_new = poly.fit_transform(X)\n",
    "    X_new = X_new[:,1:]\n",
    "    #X_new = np.column_stack((X, X[:, 0] ** 2 + X[:, 1] ** 2))\n",
    "    \n",
    "    return X_new\n",
    "\n",
    "def cube_transform(X):\n",
    "    '''\n",
    "    f(x_1, ..., x_d) = (x_1, ... x_d, x_1^2, x_1x_2, ..., x_d^2)\n",
    "    '''\n",
    "    poly = PolynomialFeatures(3)\n",
    "    X_new = poly.fit_transform(X)\n",
    "    X_new = X_new[:,1:]\n",
    "    #X_new = np.column_stack((X, X[:, 0] ** 2 + X[:, 1] ** 2))\n",
    "    \n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.linspace(0, 9, 10)\n",
    "b = np.linspace(0, 9, 10)\n",
    "\n",
    "xx, yy = np.meshgrid(a, b)\n",
    "#print(xx, yy)\n",
    "#plt.scatter(xx, yy)\n",
    "s = np.column_stack((xx.ravel(), yy.ravel()))\n",
    "s = square_transform(s)\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_line_distanse(coeff, free_coeff, x):\n",
    "\n",
    "    numerator, denominator  = 0, 0\n",
    "    for i in range(len(coeff)):\n",
    "        numerator += coeff[i] * x[i]\n",
    "    numerator += free_coeff\n",
    "\n",
    "    for i in range(len(coeff)):\n",
    "        denominator += coeff[i] ** 2\n",
    "    \n",
    "    return abs(numerator) / np.sqrt(denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearSVM(BaseEstimator):\n",
    "    def __init__(self, C, transform=None):\n",
    "        self.C = C\n",
    "        self.transform = transform\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Функция обучения модели.\n",
    "        \"\"\"\n",
    "        M, n = X.shape\n",
    "        self.weights_ = np.empty(M)\n",
    "        self.bias_ = 0\n",
    "        y[np.where(y==0)] = -1\n",
    "        self.cords_ = X  # для .draw()\n",
    "        self.labels_ = y\n",
    "        \n",
    "        self.zeros_ = np.zeros((M, M))\n",
    "        P = np.diag(np.ones(n + 1))\n",
    "        P[-1, -1] = 0\n",
    "        P = np.hstack((P, np.zeros((n + 1, M))))\n",
    "        P = np.vstack((P, np.zeros((M, M + n + 1))))\n",
    "\n",
    "        q = np.concatenate((np.zeros(n + 1), np.ones(M) * self.C))\n",
    "        \n",
    "        G = np.concatenate(((-y * X.T).T, np.array([-y]).T), axis=1)\n",
    "        G = np.vstack((G, np.zeros((M, n + 1))))\n",
    "        self.doble_ones_ = np.vstack((np.diag(-np.ones(M)), np.diag(-np.ones(M))))\n",
    "        G = np.hstack((G, self.doble_ones_))\n",
    "        \n",
    "        h = np.concatenate((-np.ones(M), np.zeros(M)))\n",
    "        \n",
    "        P = matrix(P, size=(n + 1 + M, n + 1 + M), tc='d')\n",
    "        q = matrix(q, tc='d')\n",
    "        G = matrix(G, size=(2 * M, n + 1 + M), tc='d')\n",
    "        h = matrix(h, tc='d')\n",
    "\n",
    "\n",
    "        solution = np.array(qp(P=P, q=q, G=G, h=h)['x'])\n",
    "        self.weights_ = solution[:-M-1]\n",
    "        self.bias_ = solution[n]\n",
    "        #self.bias_ = np.median(np.dot(self.weights_, X) - y)\n",
    "        self.M_, self.n_ = M, n\n",
    "        #return self.weights_, self.bias_\n",
    "\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        return (np.dot(X, self.weights_) + self.bias_).flatten()\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return np.sign(self.predict_proba(X))\n",
    "\n",
    "    def draw(self, ax_i, ax_j, *, title=None, axis_names=None):\n",
    "        '''\n",
    "        Plots cool beautiful shockingly magnificent graph.\n",
    "        I spent 5 hours figuring this out, so yeah.\n",
    "\n",
    "        Params:\n",
    "        ax_i and ax_j is coordinates for picture in plt.subplots\n",
    "        Title is title, and axis_names is the names of the axises\n",
    "        \n",
    "        '''\n",
    "\n",
    "        xvals = np.linspace(self.cords_[:, 0].min() - 1, self.cords_[:, 0].max() + 1, int(self.M_))\n",
    "        yvals = np.linspace(self.cords_[:, 1].min() - 1, self.cords_[:, 1].max() + 1, int(self.M_))\n",
    "\n",
    "        space = np.meshgrid(xvals, yvals)\n",
    "        #print(space)\n",
    "        space = np.column_stack((space[0].ravel(), space[1].ravel()))\n",
    "        if self.transform == 'square':\n",
    "            space = square_transform(space)\n",
    "        elif self.transform == 'cube':\n",
    "            space = cube_transform(space)\n",
    "\n",
    "        # zz = []\n",
    "        # for m in range(self.M_ ** 2):\n",
    "        #     zz.append(svm.predict_proba(space[m]))\n",
    "\n",
    "        zz = self.predict_proba(space)\n",
    "        xx, yy = xvals, yvals\n",
    "        \n",
    "        zz = np.array(zz).reshape(int(self.M_), int(self.M_))\n",
    "        ax[ax_i, ax_j].pcolormesh(xx, yy, zz, shading='gouraud', cmap='viridis')\n",
    "\n",
    "        margin = 1 / np.linalg.norm(self.weights_)\n",
    "\n",
    "        ax[ax_i, ax_j].contour(xx, yy, zz, levels=(0), colors='k', linewidths=1.5, zorder=1)\n",
    "        ax[ax_i, ax_j].contour(xx, yy, zz, levels=([-1, 1]), colors='white', linewidths=1.5, zorder=2, linestyles='dotted')\n",
    "        \n",
    "        ax[ax_i, ax_j].scatter(self.cords_[:, 0], self.cords_[:, 1], marker=\"o\", c=self.labels_, s=25, cmap='viridis', edgecolor=\"k\");\n",
    "        \n",
    "        support_vectors = []\n",
    "        support_labels = []\n",
    "        for i in range(self.M_):\n",
    "            if dot_line_distanse(self.weights_, self.bias_, self.cords_[i]) <= margin + 10 ** -4:\n",
    "                support_vectors.append(self.cords_[i])\n",
    "                support_labels.append(self.labels_[i])\n",
    "        support_vectors = np.array(support_vectors)\n",
    "\n",
    "        if len(support_vectors):\n",
    "            ax[ax_i, ax_j].scatter(support_vectors[:, 0], support_vectors[:, 1], marker=\"o\", c=support_labels,\n",
    "                                   s=25, cmap='viridis', edgecolor=\"1\")\n",
    "        \n",
    "        ax[ax_i, ax_j].set_ylim(self.cords_[:, 1].min() - 1, self.cords_[:, 1].max() + 1)\n",
    "        ax[ax_i, ax_j].set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearSVM(BaseEstimator):\n",
    "    def __init__(self, C, transform=None):\n",
    "        self.C = C\n",
    "        self.transform = transform\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Функция обучения модели.\n",
    "        \"\"\"\n",
    "        M, n = X.shape\n",
    "        self.weights_ = np.empty(M)\n",
    "        self.bias_ = 0\n",
    "        y[np.where(y==0)] = -1\n",
    "        self.cords_ = X  # для .draw()\n",
    "        self.labels_ = y\n",
    "        \n",
    "        self.zeros_ = np.zeros((M, M))\n",
    "        P = np.diag(np.ones(n + 1))\n",
    "        P[-1, -1] = 0\n",
    "        P = np.hstack((P, np.zeros((n + 1, M))))\n",
    "        P = np.vstack((P, np.zeros((M, M + n + 1))))\n",
    "\n",
    "        q = np.concatenate((np.zeros(n + 1), np.ones(M) * self.C))\n",
    "        \n",
    "        G = np.concatenate(((-y * X.T).T, np.array([-y]).T), axis=1)\n",
    "        G = np.vstack((G, np.zeros((M, n + 1))))\n",
    "        self.doble_ones_ = np.vstack((np.diag(-np.ones(M)), np.diag(-np.ones(M))))\n",
    "        G = np.hstack((G, self.doble_ones_))\n",
    "        \n",
    "        h = np.concatenate((-np.ones(M), np.zeros(M)))\n",
    "        \n",
    "        P = matrix(P, size=(n + 1 + M, n + 1 + M), tc='d')\n",
    "        q = matrix(q, tc='d')\n",
    "        G = matrix(G, size=(2 * M, n + 1 + M), tc='d')\n",
    "        h = matrix(h, tc='d')\n",
    "\n",
    "\n",
    "        solution = np.array(qp(P=P, q=q, G=G, h=h)['x'])\n",
    "        self.weights_ = solution[:-M-1]\n",
    "        self.bias_ = solution[n]\n",
    "        #self.bias_ = np.median(np.dot(self.weights_, X) - y)\n",
    "        self.M_, self.n_ = M, n\n",
    "        #return self.weights_, self.bias_\n",
    "\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        return (np.dot(X, self.weights_) + self.bias_).flatten()\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return np.sign(self.predict_proba(X))\n",
    "\n",
    "    def draw(self, ax_i, ax_j, *, title=None, axis_names=None):\n",
    "        '''\n",
    "        Plots cool beautiful shockingly magnificent graph.\n",
    "        I spent 5 hours figuring this out, so yeah.\n",
    "\n",
    "        Params:\n",
    "        ax_i and ax_j is coordinates for picture in plt.subplots\n",
    "        Title is title, and axis_names is the names of the axises\n",
    "        \n",
    "        '''\n",
    "\n",
    "        xvals = np.linspace(self.cords_[:, 0].min() - 1, self.cords_[:, 0].max() + 1, int(self.M_))\n",
    "        yvals = np.linspace(self.cords_[:, 1].min() - 1, self.cords_[:, 1].max() + 1, int(self.M_))\n",
    "\n",
    "        space = np.meshgrid(xvals, yvals)\n",
    "        #print(space)\n",
    "        space = np.column_stack((space[0].ravel(), space[1].ravel()))\n",
    "        if self.transform == 'square':\n",
    "            space = square_transform(space)\n",
    "        elif self.transform == 'cube':\n",
    "            space = cube_transform(space)\n",
    "\n",
    "        # zz = []\n",
    "        # for m in range(self.M_ ** 2):\n",
    "        #     zz.append(svm.predict_proba(space[m]))\n",
    "\n",
    "        zz = self.predict_proba(space)\n",
    "        xx, yy = xvals, yvals\n",
    "        \n",
    "        zz = np.array(zz).reshape(int(self.M_), int(self.M_))\n",
    "        ax[ax_i, ax_j].pcolormesh(xx, yy, zz, shading='gouraud', cmap='viridis')\n",
    "\n",
    "        margin = 1 / np.linalg.norm(self.weights_)\n",
    "\n",
    "        ax[ax_i, ax_j].contour(xx, yy, zz, levels=(0), colors='k', linewidths=1.5, zorder=1)\n",
    "        ax[ax_i, ax_j].contour(xx, yy, zz, levels=([-1, 1]), colors='white', linewidths=1.5, zorder=2, linestyles='dotted')\n",
    "        \n",
    "        ax[ax_i, ax_j].scatter(self.cords_[:, 0], self.cords_[:, 1], marker=\"o\", c=self.labels_, s=25, cmap='viridis', edgecolor=\"k\");\n",
    "        \n",
    "        support_vectors = []\n",
    "        support_labels = []\n",
    "        for i in range(self.M_):\n",
    "            if dot_line_distanse(self.weights_, self.bias_, self.cords_[i]) <= margin + 10 ** -4:\n",
    "                support_vectors.append(self.cords_[i])\n",
    "                support_labels.append(self.labels_[i])\n",
    "        support_vectors = np.array(support_vectors)\n",
    "\n",
    "        if len(support_vectors):\n",
    "            ax[ax_i, ax_j].scatter(support_vectors[:, 0], support_vectors[:, 1], marker=\"o\", c=support_labels,\n",
    "                                   s=25, cmap='viridis', edgecolor=\"1\")\n",
    "        \n",
    "        ax[ax_i, ax_j].set_ylim(self.cords_[:, 1].min() - 1, self.cords_[:, 1].max() + 1)\n",
    "        ax[ax_i, ax_j].set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Двойственный переход и Ядровой SVM\n",
    "\n",
    "Задачу обучения линейного SVM, рассмотренную в предыдущем пункте принято называть _прямой_ оптимизационной задачей для SVM. Любая задача оптимизации с ограничениями имеет [_двойственную_ задачу Лагранжа](http://goo.gl/OujTPr), в которой оптимизируются _двойственные переменные_ (множители Лагранжа), соответствующие штрафу за нарушение ограничений, максимизируется нижняя оценка функционала прямой задачи. В случае задачи квадратичного программирования, решение двойственной задачи (значение оптимизируемого функционала) совпадает с оптимумом прямой задачи.\n",
    "\n",
    "Подробнее можно почитать в [статье](./SMAIS11_SVM.pdf).\n",
    "\n",
    "Двойственная задача для SVM имеет вид:\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\sum_{n} \\alpha_n - \\frac{1}{2}\\sum_{n}\\sum_{n'} \\alpha_{n}\\alpha_{n'} y_{n}y_{n'} x_{n}^Tx_{n'} \\to \\max_{\\alpha} \\\\\n",
    "    \\begin{aligned}\n",
    "        \\text{s.t. } \\quad  \n",
    "        & 0 \\le \\alpha_n \\le C, \\quad \\forall n = 1, \\dots, N \\\\\n",
    "        & \\sum_{n} \\alpha_n y_n = 0\n",
    "    \\end{aligned}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Оптимизируется вектор из двойственных переменных $\\alpha_n$, соответствующих объектам обучающей выборки. Объект $x_n$ является опорным, если $\\alpha_n > 0$.\n",
    "\n",
    "Предсказание вычисляется по следующему правилу:\n",
    "$$\\hat{y}(x) = \\text{sign}\\left(\\sum_{n}\\alpha_{n}y_{n}x^Tx_{n} + b\\right).$$\n",
    "\n",
    "Для предсказания необходимо оценить значение $b$. Известно, что для любого опорного объекта, который классифицируется безошибочно верно:\n",
    "$$y_n = \\sum_{n'}\\alpha_{n}y_{n}x_{n}^Tx_{n'} + b,$$\n",
    "значит для любого такого объекта:\n",
    "$$b = y_n - \\sum_{n'}\\alpha_{n}y_{n}x_{n}^Tx_{n'}.$$\n",
    "\n",
    "В случае наличия ошибок классификации обучающей выборки, предлагается усреднять значение $b$ по всем опорным векторам:\n",
    "$$b = \\frac{1}{N_\\text{SV}}\\sum_{n \\in \\text{SV}}\\left(y_n - \\sum_{n'}\\alpha_{n}y_{n}x_{n}^Tx_{n'}\\right).$$\n",
    "Интуиция здесь такова, что суммарные ошибки в положительную сторону примерно равны суммарным ошибкам в отрицательную сторону.\n",
    "\n",
    "Другой вариант — отказаться от параметра $b$ и работать с моделью $f(x) = w^Tx$, добавив к вектору $x$ константный признак.\n",
    "\n",
    "#### Неявное преобразование признаков\n",
    "Отметим, что двойственная задача SVM содержит вектора признаков исключительно в виде скалярного произведения $x^Tx'$. Эта особенность позволяет производить неявное преобразование признакового пространства. Вместо вычисления функции $\\phi(\\boldsymbol x)$, которая может отображать исходные признаки в вектора очень большой размерности, будем вычислять скалярное произведение $k(\\boldsymbol x, \\boldsymbol x') = \\phi(\\boldsymbol x)^T\\phi(\\boldsymbol x')$ называемое _ядром_. \n",
    "\n",
    "#### Задание\n",
    "  1. Реализуйте процедуру обучения ядрового SVM, используя QP-солвер.\n",
    "  2. Протестируйте на случайных двумерных выборках ядровой SVM. Покажите на плоскости строящиеся разделяющие поверхности, линии уровня, ограничивающие коридор $f(\\boldsymbol x) = \\pm 1$. Выделите опорные вектора точками другой формы или большего размера. Попробуйте следующие ядровые функции:\n",
    "    - линейная: $k(x, x') = x^Tx'$\n",
    "    - полиномиальная: $k(x, x') = (x^Tx' + 1)^p$ с различными степенями $p = 2,3,\\dots$\n",
    "    - Гауссовская-RBF: $k(x, x') = \\exp(-\\frac{1}{2\\gamma}\\|x - x'\\|^2)$\n",
    "  3. Как ведет себя SVM с полиномиальным ядром в зависимости от параметров $C$ и степени ядра $p$?\n",
    "  4. Как ведет себя SVM с RBF-ядром в зависимости от параметров $C$ и $\\gamma$? Поварьируйте параметры $C$ и $\\gamma$ по логарифмической сетке. Какие значения параметров ведут к переобучению, а какие — к слишком грубой модели?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Примеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настройка вывода графиков [`Maplotlib`](http://matplotlib.org/) и импорт функций из модуля [`pylab`](http://wiki.scipy.org/PyLab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['matrix']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуальное решение задачи квадратичного программирования\n",
    "\n",
    "На следующем рисунке наглядно показано решение задачи QP из задания 1. Оптимизируемая функция $f(\\boldsymbol x)$ показана линиями уровня, область значений недопустимых ограничениями окрашена в серый цвет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([True, False, False]) & np.array([True, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logical_and(np.array([True, False, False]), np.array([True, False, True]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка и использование `CvxOpt`\n",
    "\n",
    "Библиотека [`cvxopt`](http://cvxopt.org/) может быть установлена как обычный python-пакет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cvxopt\n",
      "  Using cached https://files.pythonhosted.org/packages/16/a0/0d090735e2639a74d6628831e02cc59284e3a3a4f5910f496fc6e435b645/cvxopt-1.2.5-cp36-cp36m-win_amd64.whl\n",
      "Collecting mkl (from cvxopt)\n",
      "  Using cached https://files.pythonhosted.org/packages/56/39/537cb3e4e93f1ac5085dc3b3a43cfd99d0af9b29c44fcaa99490f526b611/mkl-2019.0-py2.py3-none-win_amd64.whl\n",
      "Requirement already satisfied, skipping upgrade: intel-openmp in c:\\anaconda3\\lib\\site-packages (from mkl->cvxopt) (2019.0)\n",
      "Installing collected packages: mkl, cvxopt\n",
      "Successfully installed cvxopt-1.2.5 mkl-2019.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --user cvxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нас будет интересовать функция [`cvxopt.solvers.qp()`](http://cvxopt.org/examples/tutorial/qp.html):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
